{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Differential Privacy for Deep Learning\n",
    "\n",
    "So in the last lessons you may have been wondering - what does all of this have to do with Deep Learning? Well, these same techniques we were just studying form the core primitives for how Differential Privacy provides guarantees in the context of Deep Learning. \n",
    "\n",
    "Previously, we defined perfect privacy as \"a query to a database returns the same value even if we remove any person from the database\", and used this intuition in the description of epsilon/delta. In the context of deep learning we have a similar standard.\n",
    "\n",
    "Training a model on a dataset should return the same model even if we remove any person from the dataset.\n",
    "\n",
    "Thus, we've replaced \"querying a database\" with \"training a model on a dataset\". In essence, the training process is a kind of query. However, one should note that this adds two points of complexity which database queries did not have:\n",
    "\n",
    "    1. do we always know where \"people\" are referenced in the dataset?\n",
    "    2. neural models rarely never train to the same output model, even on identical data\n",
    "\n",
    "The answer to (1) is to treat each training example as a single, separate person. Strictly speaking, this is often overly zealous as some training examples have no relevance to people and others may have multiple/partial (consider an image with multiple people contained within it). Thus, localizing exactly where \"people\" are referenced, and thus how much your model would change if people were removed, is challenging.\n",
    "\n",
    "The answer to (2) is also an open problem - but several interesitng proposals have been made. We're going to focus on one of the most popular proposals, PATE.\n",
    "\n",
    "## An Example Scenario: A Health Neural Network\n",
    "\n",
    "First we're going to consider a scenario - you work for a hospital and you have a large collection of images about your patients. However, you don't know what's in them. You would like to use these images to develop a neural network which can automatically classify them, however since your images aren't labeled, they aren't sufficient to train a classifier. \n",
    "\n",
    "However, being a cunning strategist, you realize that you can reach out to 10 partner hospitals which DO have annotated data. It is your hope to train your new classifier on their datasets so that you can automatically label your own. While these hospitals are interested in helping, they have privacy concerns regarding information about their patients. Thus, you will use the following technique to train a classifier which protects the privacy of patients in the other hospitals.\n",
    "\n",
    "- 1) You'll ask each of the 10 hospitals to train a model on their own datasets (All of which have the same kinds of labels)\n",
    "- 2) You'll then use each of the 10 partner models to predict on your local dataset, generating 10 labels for each of your datapoints\n",
    "- 3) Then, for each local data point (now with 10 labels), you will perform a DP query to generate the final true label. This query is a \"max\" function, where \"max\" is the most frequent label across the 10 labels. We will need to add laplacian noise to make this Differentially Private to a certain epsilon/delta constraint.\n",
    "- 4) Finally, we will retrain a new model on our local dataset which now has labels. This will be our final \"DP\" model.\n",
    "\n",
    "So, let's walk through these steps. I will assume you're already familiar with how to train/predict a deep neural network, so we'll skip steps 1 and 2 and work with example data. We'll focus instead on step 3, namely how to perform the DP query for each example using toy data.\n",
    "\n",
    "So, let's say we have 10,000 training examples, and we've got 10 labels for each example (from our 10 \"teacher models\" which were trained directly on private data). Each label is chosen from a set of 10 possible labels (categories) for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTeachers = 10 #10 partner Hospitals\n",
    "numExamples = 10000 #Size of our DataSet\n",
    "numLabels = 10 #number of labels for our classifer and we want 1 out of these 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (np.random.rand(numTeachers, numExamples) * numLabels).astype(int).transpose(1,0) #Fake Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84784545 0.04762735]] [[8.47845447 0.47627352]] [[8 0]]\n"
     ]
    }
   ],
   "source": [
    "#Just to have an insight how random.rand works like\n",
    "x = np.random.rand(1,2)\n",
    "y = x * 10\n",
    "z = y.astype(int)\n",
    "print(x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 4, 3, 1, 4, 8, 0, 8, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]#Prediction of an entry by all teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anImage = preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6, 4, 3, 1, 4, 8, 0, 8, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we gonna take the above vector and transfrom into single label. We will just take the prediction occuring most of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCounts = np.bincount(anImage, minlength=numLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(labelCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 1, 1, 0, 2, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCounts\n",
    "#Remember we have to take the prediction with max occurance not just max number i.e. we did bincount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not Differentially Private, so we are gonna add noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "beta = 1 / epsilon\n",
    "\n",
    "for i in range(len(labelCounts)):\n",
    "    labelCounts[i] += np.random.laplace(0, beta, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6, -10,  18,  43,  12, -14,  10,  -7,  -1,  12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(labelCounts)#dont worry that it changed the label, Our DNN would take care of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLabels = []\n",
    "for anImage in preds:\n",
    "    \n",
    "    labelCounts = np.bincount(anImage, minlength=numLabels)\n",
    "    \n",
    "    epsilon = 0.1\n",
    "    beta = 1 / epsilon\n",
    "    \n",
    "    for i in range(len(labelCounts)):\n",
    "        labelCounts[i] += np.random.laplace(0, beta, 1)\n",
    "        \n",
    "    newLabel = np.argmax(labelCounts)\n",
    "    newLabels.append(newLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTeachers, numExamples, numLabels = (100,100,10)\n",
    "preds = (np.random.rand(numTeachers, numExamples) * numLabels).astype(int)#fake preds\n",
    "indices = (np.random.rand(numExamples) * numLabels).astype(int)#true answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dep_eps, data_indep_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1,delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent epsilon :  11.756462732485115\n",
      "Data Dependent epsilon :  11.756462732485105\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Independent epsilon : \",data_indep_eps) #Slightly higher\n",
    "print(\"Data Dependent epsilon : \",data_dep_eps)\n",
    "#There's tini tiny tiny tiny agreement b/w the models. well it's not surprising coz as we randomly generated the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,0:5] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent epsilon :  11.756462732485115\n",
      "Data Dependent epsilon :  7.867987172744541\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_indep_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1,delta=1e-5)\n",
    "print(\"Data Independent epsilon : \",data_indep_eps)\n",
    "print(\"Data Dependent epsilon : \",data_dep_eps)\n",
    "#Ohh this dataset leaks only 7.8 epsilons as opposed to 11.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,0:50] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: May not have used enough values of l. Increase 'moments' variable and run again.\n",
      "Data Independent epsilon :  11.756462732485115\n",
      "Data Dependent epsilon :  1.52655213289881\n"
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_indep_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1,delta=1e-5)\n",
    "print(\"Data Independent epsilon : \",data_indep_eps)\n",
    "print(\"Data Dependent epsilon : \",data_dep_eps)\n",
    "# -.-  Significantly better privacy leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More the predoctions agree with each other, the tighter the value of epsilon we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
