{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Deep learning networks tend to be massive with dozens or hundreds of layers, that's where the term \"deep\" comes from. You can build one of these deep networks using only weight matrices as we did in the previous notebook, but in general it's very cumbersome and difficult to implement. PyTorch has a nice module `nn` that provides a nice way to efficiently build large neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets , transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a transformer to normalise the data\n",
    "transformer = transforms.Compose([transforms.ToTensor() , transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = datasets.MNIST('MNIST_data' , download=True , train=True , transform=transformer)\n",
    "trainLoader = torch.utils.data.dataloader(trainSet , batch_size = 64 , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_Data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "You'll notice I created the `trainloader` with a batch size of 64, and `shuffle=True`. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a *batch*. And `shuffle=True` tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size `(64, 1, 28, 28)`. So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images , labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor -> Numpy -> Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x105aea9b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze() , cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "> Flatten the batch of images `images`. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "#Flattening\n",
    "inputs = images.view(images.shape[0],-1)#shape0 gives batches\n",
    "inputUnits = 784\n",
    "hiddenUnits = 256\n",
    "outputUnits = 10\n",
    "\n",
    "w1 = torch.randn(inputUnits , hiddenUnits)\n",
    "b1 = torch.randn(hiddenUnits)\n",
    "\n",
    "w2 = torch.randn(hiddenUnits , outputUnits)\n",
    "b2 = torch.randn(outputUnits)\n",
    "\n",
    "h = activation(torch.mm(inputs , w1) + b1)\n",
    "\n",
    "out = torch.mm(h , w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape #BatchSize 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(out)\n",
    "def softmaxActivation(x):\n",
    "    #print(x.shape) = > 64 x 10\n",
    "    # we want the sum of exp along a row\n",
    "    numerator = torch.exp(x)\n",
    "    print(numerator.shape)\n",
    "    denominator = torch.sum(torch.exp(x) , dim = 1).view(-1,1)# View for converting 1D to 2D\n",
    "    print(denominator.shape)\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "probabilities = softmaxActivation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.sum(dim=1)# Summing up all the values in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "PyTorch provides a module `nn` that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()#of nn.module # To link this class to the nn.Module\n",
    "        # Without this you won't be able to do anything\n",
    "        \n",
    "        '''nn.Linear will multiply the weights and add bias'''\n",
    "        \n",
    "        #Inputs to hidden layer Linear transformation\n",
    "        self.hidden = nn.Linear(784 , 256)\n",
    "        \n",
    "        #Hidden to Output layer Linear transformation\n",
    "        self.output = nn.Linear(256 , 10)\n",
    "        \n",
    "        \n",
    "        #Defining sigmoid activation & softmax output\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1) #for taking sum across row\n",
    "        \n",
    "    def forward(self , x):\n",
    "        # Pass the input tensor through each of the operations\n",
    "        \n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating object of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > making use of\n",
    " # F\n",
    " > For Cleaner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Hidden layer with sigmoid\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        \n",
    "        #Output layer with softmax\n",
    "        x = F.softmax(self.output(x) , dim=1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2\n",
    "> Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the `nn.ReLU` module or `F.relu` function.\n",
    "\n",
    "It's good practice to name your layers by their type of network, for instance 'fc' to represent a fully-connected layer. As you code your solution, use `fc1`, `fc2`, and `fc3` as your layer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ownNetork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784 , 128)\n",
    "        self.fc2 = nn.Linear(128 , 64)\n",
    "        self.fc3 = nn.Linear(64 , 10)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.softmax(self.fc3(x) , dim=1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ownObject = ownNetork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ownNetork(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ownObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0162, -0.0017,  0.0311,  ..., -0.0126, -0.0175,  0.0170],\n",
       "        [-0.0191,  0.0190, -0.0200,  ...,  0.0073,  0.0328,  0.0048],\n",
       "        [-0.0077,  0.0104,  0.0215,  ...,  0.0014, -0.0276,  0.0252],\n",
       "        ...,\n",
       "        [-0.0031, -0.0103, -0.0145,  ...,  0.0344, -0.0301,  0.0308],\n",
       "        [-0.0001,  0.0233,  0.0086,  ...,  0.0218, -0.0351,  0.0231],\n",
       "        [-0.0343, -0.0005, -0.0354,  ..., -0.0036, -0.0031,  0.0181]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ownObject.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0114, -0.0020,  0.0085,  0.0228, -0.0058,  0.0242, -0.0277,  0.0087,\n",
       "         0.0220, -0.0049, -0.0131, -0.0303, -0.0021,  0.0341,  0.0178, -0.0015,\n",
       "         0.0227, -0.0142, -0.0219,  0.0055, -0.0043, -0.0219,  0.0031, -0.0109,\n",
       "        -0.0308, -0.0127,  0.0221, -0.0013, -0.0093, -0.0125,  0.0319, -0.0321,\n",
       "        -0.0016, -0.0068, -0.0012,  0.0175, -0.0186,  0.0244,  0.0131, -0.0031,\n",
       "         0.0025,  0.0312, -0.0277, -0.0204,  0.0276,  0.0038, -0.0270, -0.0110,\n",
       "         0.0139,  0.0032, -0.0168,  0.0203, -0.0027, -0.0114, -0.0035, -0.0233,\n",
       "         0.0004,  0.0190,  0.0034, -0.0067, -0.0111, -0.0254, -0.0058,  0.0113,\n",
       "         0.0299, -0.0202,  0.0247,  0.0043, -0.0335, -0.0342, -0.0126, -0.0310,\n",
       "         0.0309,  0.0230, -0.0034, -0.0259,  0.0036,  0.0331, -0.0155, -0.0094,\n",
       "         0.0325, -0.0340, -0.0166,  0.0228,  0.0050, -0.0201, -0.0234,  0.0160,\n",
       "        -0.0304, -0.0122, -0.0201, -0.0257,  0.0168, -0.0333, -0.0351,  0.0197,\n",
       "         0.0071,  0.0275,  0.0111,  0.0178,  0.0286, -0.0323,  0.0306,  0.0021,\n",
       "         0.0014, -0.0254,  0.0233,  0.0206, -0.0021,  0.0212, -0.0065, -0.0150,\n",
       "         0.0145,  0.0009, -0.0159,  0.0302,  0.0218,  0.0329, -0.0154,  0.0158,\n",
       "        -0.0164,  0.0018, -0.0198, -0.0014,  0.0238, -0.0005,  0.0228, -0.0128],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ownObject.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set bias to Zero #INplace\n",
    "ownObject.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0074,  0.0052, -0.0085,  ...,  0.0031,  0.0044,  0.0181],\n",
       "        [-0.0016, -0.0027, -0.0009,  ...,  0.0019, -0.0026, -0.0037],\n",
       "        [ 0.0092, -0.0240, -0.0092,  ...,  0.0186,  0.0009, -0.0124],\n",
       "        ...,\n",
       "        [ 0.0156,  0.0130,  0.0064,  ..., -0.0276,  0.0131, -0.0066],\n",
       "        [-0.0170,  0.0056, -0.0024,  ...,  0.0064,  0.0155, -0.0039],\n",
       "        [-0.0008,  0.0106, -0.0016,  ..., -0.0101,  0.0243, -0.0060]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "ownObject.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images , labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape , labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.resize_(64 , 1 , 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the network\n",
    "\n",
    "imageIndex = 0\n",
    "ps = ownObject.forward(images[imageIndex , :])\n",
    "\n",
    "img = images[imageIndex]\n",
    "\n",
    "#helper.view_classify(img.view(-1,28,28) , ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b1f6b00>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0940, 0.0932, 0.0977, 0.1147, 0.1030, 0.0919, 0.0928, 0.0997, 0.0988,\n",
       "         0.1142]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!\n",
    "\n",
    "### Using `nn.Sequential`\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "#helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations are availble by passing in the appropriate index. For example, if you want to get first Linear operation and look at the weights, you'd use `model[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0198, -0.0199, -0.0197,  ...,  0.0020,  0.0073,  0.0318],\n",
       "        [ 0.0156, -0.0081, -0.0092,  ..., -0.0090, -0.0275, -0.0105],\n",
       "        [ 0.0182, -0.0002,  0.0219,  ..., -0.0310, -0.0274, -0.0304],\n",
       "        ...,\n",
       "        [-0.0096, -0.0315,  0.0340,  ...,  0.0285, -0.0199,  0.0248],\n",
       "        [ 0.0076,  0.0346, -0.0193,  ..., -0.0057,  0.0160,  0.0213],\n",
       "        [-0.0311, -0.0308, -0.0265,  ..., -0.0117,  0.0014,  0.0075]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model[0])\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=784, out_features=128, bias=True),\n",
       " Linear(in_features=128, out_features=64, bias=True))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0] , model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=784, out_features=128, bias=True),\n",
       " Linear(in_features=128, out_features=64, bias=True))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1 , model.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
